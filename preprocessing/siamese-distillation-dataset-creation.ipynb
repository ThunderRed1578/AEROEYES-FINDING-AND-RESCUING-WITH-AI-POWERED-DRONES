{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13557915,"sourceType":"datasetVersion","datasetId":8611691},{"sourceId":13563608,"sourceType":"datasetVersion","datasetId":8615623},{"sourceId":13696631,"sourceType":"datasetVersion","datasetId":8712216},{"sourceId":660675,"sourceType":"modelInstanceVersion","modelInstanceId":499713,"modelId":514941}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Kaggle URL: [Siamese Distillation Dataset Creation](https://www.kaggle.com/code/phatle1578/siamese-distillation-dataset-creation)","metadata":{}},{"cell_type":"code","source":"# !pip -q uninstall -y numpy scipy scikit-learn tensorflow keras\n# !pip -q install ultralytics==8.3.27 numpy==1.26.4 scipy==1.11.4 scikit-learn==1.3.2 open_clip_torch==2.24.0\n!pip -q install ultralytics==8.3.27 open_clip_torch==2.24.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:02:46.024718Z","iopub.execute_input":"2025-12-25T04:02:46.024936Z","iopub.status.idle":"2025-12-25T04:02:58.355938Z","shell.execute_reply.started":"2025-12-25T04:02:46.024909Z","shell.execute_reply":"2025-12-25T04:02:58.354847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/IDEA-Research/GroundingDINO.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:02:58.358577Z","iopub.execute_input":"2025-12-25T04:02:58.35925Z","iopub.status.idle":"2025-12-25T04:03:00.626773Z","shell.execute_reply.started":"2025-12-25T04:02:58.359213Z","shell.execute_reply":"2025-12-25T04:03:00.625722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/GroundingDINO/\n!pip install -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:03:00.628073Z","iopub.execute_input":"2025-12-25T04:03:00.628335Z","iopub.status.idle":"2025-12-25T04:03:55.127862Z","shell.execute_reply.started":"2025-12-25T04:03:00.628312Z","shell.execute_reply":"2025-12-25T04:03:55.12683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir weights\n%cd weights\n!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:03:55.129302Z","iopub.execute_input":"2025-12-25T04:03:55.129999Z","iopub.status.idle":"2025-12-25T04:03:59.928968Z","shell.execute_reply.started":"2025-12-25T04:03:55.129953Z","shell.execute_reply":"2025-12-25T04:03:59.928048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy, scipy\nprint(numpy.__version__)\nprint(scipy.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:03:59.930294Z","iopub.execute_input":"2025-12-25T04:03:59.930568Z","iopub.status.idle":"2025-12-25T04:03:59.979892Z","shell.execute_reply.started":"2025-12-25T04:03:59.930545Z","shell.execute_reply":"2025-12-25T04:03:59.979133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os, json\nimport requests\nimport math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport re\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:03:59.981044Z","iopub.execute_input":"2025-12-25T04:03:59.981263Z","iopub.status.idle":"2025-12-25T04:04:02.046335Z","shell.execute_reply.started":"2025-12-25T04:03:59.981244Z","shell.execute_reply":"2025-12-25T04:04:02.045568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -V","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:04:02.049781Z","iopub.execute_input":"2025-12-25T04:04:02.050125Z","iopub.status.idle":"2025-12-25T04:04:03.078542Z","shell.execute_reply.started":"2025-12-25T04:04:02.050102Z","shell.execute_reply":"2025-12-25T04:04:03.07748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/GroundingDINO\nfrom groundingdino.util.inference import load_model, load_image, predict, annotate\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndino_model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/groundingdino_swint_ogc.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:04:03.081262Z","iopub.execute_input":"2025-12-25T04:04:03.08156Z","iopub.status.idle":"2025-12-25T04:04:29.781794Z","shell.execute_reply.started":"2025-12-25T04:04:03.081537Z","shell.execute_reply":"2025-12-25T04:04:29.780983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:04:29.782911Z","iopub.execute_input":"2025-12-25T04:04:29.783177Z","iopub.status.idle":"2025-12-25T04:04:40.138196Z","shell.execute_reply.started":"2025-12-25T04:04:29.783156Z","shell.execute_reply":"2025-12-25T04:04:40.137106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nimport open_clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:04:40.139986Z","iopub.execute_input":"2025-12-25T04:04:40.140758Z","iopub.status.idle":"2025-12-25T04:04:40.392128Z","shell.execute_reply.started":"2025-12-25T04:04:40.140723Z","shell.execute_reply":"2025-12-25T04:04:40.391355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Load Teacher CLIP (Để lấy tri thức đặc trưng) ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n    'ViT-L-14', pretrained='openai'\n)\nclip_model = clip_model.to(device).eval()\n\n# 1. Cấu hình đường dẫn\nDATA_ROOT = \"/kaggle/input/train-zaic-dl\"\nDISTILL_DIR = \"/kaggle/working/distill_data\"\nYOLO_IMG = f\"{DISTILL_DIR}/yolo/images\"\nYOLO_LBL = f\"{DISTILL_DIR}/yolo/labels\"\nSIAM_CROPS = f\"{DISTILL_DIR}/siamese/crops\"\nos.makedirs(YOLO_IMG, exist_ok=True)\nos.makedirs(YOLO_LBL, exist_ok=True)\nos.makedirs(SIAM_CROPS, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:04:40.393402Z","iopub.execute_input":"2025-12-25T04:04:40.393713Z","iopub.status.idle":"2025-12-25T04:05:04.70306Z","shell.execute_reply.started":"2025-12-25T04:04:40.393687Z","shell.execute_reply":"2025-12-25T04:05:04.702256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_reference_features(obj_images_dir):\n    \"\"\"Tính vector đặc trưng trung bình từ các ảnh trong folder object_images\"\"\"\n    feats = []\n    for img_name in os.listdir(obj_images_dir):\n        img_p = os.path.join(obj_images_dir, img_name)\n        img_pil = Image.open(img_p).convert(\"RGB\")\n        img_in = clip_preprocess(img_pil).unsqueeze(0).to(device)\n        with torch.no_grad():\n            f = clip_model.encode_image(img_in)\n            f /= f.norm(dim=-1, keepdim=True)\n        feats.append(f)\n    if not feats: return None\n    combined = torch.mean(torch.stack(feats), dim=0)\n    return combined / combined.norm(dim=-1, keepdim=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:05:04.704268Z","iopub.execute_input":"2025-12-25T04:05:04.704581Z","iopub.status.idle":"2025-12-25T04:05:04.710885Z","shell.execute_reply.started":"2025-12-25T04:05:04.704552Z","shell.execute_reply":"2025-12-25T04:05:04.710064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def annotate_on_frame(image_path, text_prompt, reference_features=None, box_threshold=0.35, text_threshold=0.25):\n    \"\"\"\n    Sử dụng GroundingDINO để tìm box và CLIP để lọc theo độ tương đồng (nếu có reference_features).\n    \"\"\"\n    # 1. Load và tiền xử lý ảnh cho GroundingDINO\n    image_source, image = load_image(image_path) # Hàm load_image có sẵn trong GroundingDINO repo\n\n    # 2. Dự đoán bằng GroundingDINO (Teacher Detection)\n    boxes, logits, phrases = predict(\n        model=dino_model, \n        image=image, \n        caption=text_prompt, \n        box_threshold=box_threshold, \n        text_threshold=text_threshold,\n        device=device\n    )\n\n    # 3. Chuyển đổi tọa độ từ normalized [0, 1] sang pixel tuyệt đối\n    h, w, _ = image_source.shape\n    abs_boxes = []\n    for box in boxes:\n        # box format: [cx, cy, w, h]\n        x_c, y_c, wb, hb = box * torch.Tensor([w, h, w, h])\n        x1 = int(x_c - wb/2)\n        y1 = int(y_c - hb/2)\n        x2 = int(x_c + wb/2)\n        y2 = int(y_c + hb/2)\n        abs_boxes.append([max(0, x1), max(0, y1), min(w, x2), min(h, y2)])\n\n    # 4. Cắt ảnh (Crops) để chuẩn bị cho CLIP lọc/trích xuất đặc trưng\n    crops = []\n    valid_indices = []\n    for i, box in enumerate(abs_boxes):\n        crop = image_source[box[1]:box[3], box[0]:box[2]]\n        if crop.size > 0:\n            crops.append(crop)\n            valid_indices.append(i)\n\n    # 5. Nếu có ảnh mẫu (Few-shot logic), lọc những box không giống ảnh mẫu\n    if reference_features is not None and len(crops) > 0:\n        final_boxes, final_crops, final_logits = [], [], []\n        \n        for i, crop in enumerate(crops):\n            # Trích xuất feature của crop hiện tại bằng CLIP Teacher\n            crop_pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n            crop_input = clip_preprocess(crop_pil).unsqueeze(0).to(device)\n            \n            with torch.no_grad():\n                crop_feat = clip_model.encode_image(crop_input)\n                crop_feat /= crop_feat.norm(dim=-1, keepdim=True)\n            \n            # Tính similarity với trung bình cộng feature ảnh mẫu\n            sim = torch.cosine_similarity(crop_feat, reference_features).item()\n            \n            # Chỉ giữ lại nếu similarity đủ cao (ví dụ > 0.25)\n            if sim > 0.25:\n                idx = valid_indices[i]\n                final_boxes.append(abs_boxes[idx])\n                final_crops.append(crop)\n                final_logits.append(logits[idx])\n        \n        return final_boxes, final_crops, final_logits\n    \n    # Nếu không có ảnh mẫu, trả về toàn bộ kết quả của GroundingDINO\n    return [abs_boxes[i] for i in valid_indices], crops, [logits[i] for i in valid_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:05:04.712033Z","iopub.execute_input":"2025-12-25T04:05:04.712275Z","iopub.status.idle":"2025-12-25T04:05:04.727016Z","shell.execute_reply.started":"2025-12-25T04:05:04.712257Z","shell.execute_reply":"2025-12-25T04:05:04.726155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_label_from_folder(folder_name):\n    # Loại bỏ phần số và gạch dưới ở cuối (vd: Backpack_0 -> Backpack)\n    label = re.sub(r'_\\d+$', '', folder_name)\n    # Thay thế gạch dưới giữa các từ bằng khoảng trắng (nếu có)\n    return label.replace('_', ' ')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:05:04.728084Z","iopub.execute_input":"2025-12-25T04:05:04.72833Z","iopub.status.idle":"2025-12-25T04:05:04.741941Z","shell.execute_reply.started":"2025-12-25T04:05:04.72831Z","shell.execute_reply":"2025-12-25T04:05:04.741138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"distill_metadata = []\n\nfor folder_name in sorted(os.listdir(DATA_ROOT)):\n    folder_path = os.path.join(DATA_ROOT, folder_name)\n    if not os.path.isdir(folder_path): continue\n    \n    prompt = get_label_from_folder(folder_name) # Hàm bạn đã viết\n    ref_feat = get_reference_features(os.path.join(folder_path, \"object_images\")) # CLIP ViT-L-14\n    \n    frames_path = os.path.join(folder_path, \"object_frames\")\n    for frame_name in tqdm(os.listdir(frames_path), desc=f\"Distilling {folder_name}\"):\n        img_p = os.path.join(frames_path, frame_name)\n        \n        # 1. Teacher tạo Box và lọc bằng CLIP\n        # Trả về abs_boxes [x1, y1, x2, y2]\n        boxes, crops, _ = annotate_on_frame(img_p, prompt, reference_features=ref_feat)\n        if not boxes: continue\n\n        # 2. Lưu cho YOLO11\n        img_cv2 = cv2.imread(img_p)\n        h, w, _ = img_cv2.shape\n        new_name = f\"{folder_name}_{frame_name}\"\n        cv2.imwrite(os.path.join(YOLO_IMG, new_name), img_cv2)\n        \n        with open(os.path.join(YOLO_LBL, new_name.replace(\".jpg\", \".txt\")), \"w\") as f:\n            for i, box in enumerate(boxes):\n                x1, y1, x2, y2 = box\n                # Chuyển sang format YOLO: class cx cy bw bh normalized\n                cx, cy = (x1 + x2)/2/w, (y1 + y2)/2/h\n                bw, bh = (x2 - x1)/w, (y2 - y1)/h\n                f.write(f\"0 {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\\n\")\n\n                # 3. Lưu cho Siamese (Trích xuất tri thức Teacher)\n                crop_pil = Image.fromarray(cv2.cvtColor(crops[i], cv2.COLOR_BGR2RGB))\n                crop_input = clip_preprocess(crop_pil).unsqueeze(0).to(device)\n                with torch.no_grad():\n                    teacher_emb = clip_model.encode_image(crop_input)\n                    teacher_emb = torch.nn.functional.normalize(teacher_emb, p=2, dim=1)\n                \n                crop_name = f\"{new_name.split('.')[0]}_{i}.jpg\"\n                cv2.imwrite(os.path.join(SIAM_CROPS, crop_name), crops[i])\n                distill_metadata.append({\n                    \"crop_name\": crop_name,\n                    \"embedding\": teacher_emb.cpu().numpy().tolist()[0] # 768 dims\n                })\n\nwith open(f\"{DISTILL_DIR}/siamese_metadata.json\", \"w\") as f:\n    json.dump(distill_metadata, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T04:05:04.743084Z","iopub.execute_input":"2025-12-25T04:05:04.743329Z","iopub.status.idle":"2025-12-25T04:18:22.359746Z","shell.execute_reply.started":"2025-12-25T04:05:04.743309Z","shell.execute_reply":"2025-12-25T04:18:22.358062Z"}},"outputs":[],"execution_count":null}]}