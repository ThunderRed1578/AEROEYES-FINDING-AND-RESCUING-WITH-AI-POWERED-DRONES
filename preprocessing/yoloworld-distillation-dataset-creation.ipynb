{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13563608,"sourceType":"datasetVersion","datasetId":8615623},{"sourceId":13695989,"sourceType":"datasetVersion","datasetId":8711732}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Kaggle URL: [YoloWorld Distillation Dataset Creation](https://www.kaggle.com/code/windstorm1412/yoloworld-distillation-dataset-creation)","metadata":{}},{"cell_type":"code","source":"!pip -q install ultralytics==8.3.27 open_clip_torch==2.24.0\n!pip uninstall -y ray ray[default] ray[tune] >/dev/null 2>&1 || true\n!pip install -q \"numpy<2.0\"\n!pip install -q protobuf==3.20.3 tensorboard==2.14.0\n!pip install -q filterpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:09:25.152293Z","iopub.execute_input":"2025-11-18T11:09:25.152536Z","iopub.status.idle":"2025-11-18T11:09:35.61629Z","shell.execute_reply.started":"2025-11-18T11:09:25.152503Z","shell.execute_reply":"2025-11-18T11:09:35.615495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/IDEA-Research/GroundingDINO.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:09:35.61919Z","iopub.execute_input":"2025-11-18T11:09:35.619571Z","iopub.status.idle":"2025-11-18T11:09:37.608994Z","shell.execute_reply.started":"2025-11-18T11:09:35.619534Z","shell.execute_reply":"2025-11-18T11:09:37.607947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/GroundingDINO/\n!pip install -e .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:09:37.610193Z","iopub.execute_input":"2025-11-18T11:09:37.610443Z","iopub.status.idle":"2025-11-18T11:10:24.013559Z","shell.execute_reply.started":"2025-11-18T11:09:37.61042Z","shell.execute_reply":"2025-11-18T11:10:24.012491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir weights\n%cd weights\n!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:10:24.015093Z","iopub.execute_input":"2025-11-18T11:10:24.015566Z","iopub.status.idle":"2025-11-18T11:10:29.074574Z","shell.execute_reply.started":"2025-11-18T11:10:24.015529Z","shell.execute_reply":"2025-11-18T11:10:29.073715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os, json\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport requests\nimport math\nimport torch\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:10:29.076013Z","iopub.execute_input":"2025-11-18T11:10:29.076408Z","iopub.status.idle":"2025-11-18T11:10:42.463118Z","shell.execute_reply.started":"2025-11-18T11:10:29.076362Z","shell.execute_reply":"2025-11-18T11:10:42.462434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/GroundingDINO\nfrom groundingdino.util.inference import load_model, load_image, predict, annotate\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndino_model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/groundingdino_swint_ogc.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:10:42.464467Z","iopub.execute_input":"2025-11-18T11:10:42.464796Z","iopub.status.idle":"2025-11-18T11:10:48.574646Z","shell.execute_reply.started":"2025-11-18T11:10:42.464759Z","shell.execute_reply":"2025-11-18T11:10:48.573919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:10:48.577745Z","iopub.execute_input":"2025-11-18T11:10:48.578362Z","iopub.status.idle":"2025-11-18T11:10:58.626236Z","shell.execute_reply.started":"2025-11-18T11:10:48.578331Z","shell.execute_reply":"2025-11-18T11:10:58.625078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 1","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# CELL 1: INSTALLATION\n# =============================================================================\n# Cháº¡y cell nÃ y, sau Ä‘Ã³ RESTART KERNEL trÆ°á»›c khi cháº¡y tiáº¿p\n\n!pip install -q opencv-python matplotlib\n!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\n!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:10:58.627809Z","iopub.execute_input":"2025-11-18T11:10:58.628104Z","iopub.status.idle":"2025-11-18T11:17:23.524725Z","shell.execute_reply.started":"2025-11-18T11:10:58.628077Z","shell.execute_reply":"2025-11-18T11:17:23.52349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport os\n\n# CRITICAL: Set environment variables FIRST, before any imports\nos.environ['TORCH_JIT'] = '0'\nos.environ['PYTORCH_JIT'] = '0'\n\nprint(\"Setting environment variables...\")\nprint(f\"TORCH_JIT = {os.environ.get('TORCH_JIT')}\")\nprint(f\"PYTORCH_JIT = {os.environ.get('PYTORCH_JIT')}\")\n\n# Patch SAM2 transforms.py to remove torch.jit.script\ndef patch_sam2_transforms():\n    \"\"\"Find and patch SAM2 transforms.py file\"\"\"\n    for path in sys.path:\n        transforms_file = os.path.join(path, 'sam2', 'utils', 'transforms.py')\n        if os.path.exists(transforms_file):\n            print(f\"\\nðŸ“ Found transforms.py: {transforms_file}\")\n            \n            # Read original content\n            with open(transforms_file, 'r') as f:\n                content = f.read()\n            \n            # Create backup\n            backup_file = transforms_file + '.backup'\n            if not os.path.exists(backup_file):\n                with open(backup_file, 'w') as f:\n                    f.write(content)\n                print(f\"ðŸ’¾ Backup created: {backup_file}\")\n            \n            # Check if already patched\n            if 'torch.jit.script(' in content:\n                # Patch: Remove torch.jit.script wrapper\n                original_line = 'self.transforms = torch.jit.script('\n                patched_line = 'self.transforms = ('\n                content = content.replace(original_line, patched_line)\n                \n                # Write patched version\n                with open(transforms_file, 'w') as f:\n                    f.write(content)\n                \n                print(\"âœ… SAM2 transforms.py patched successfully!\")\n                print(\"   Removed: torch.jit.script() wrapper\")\n                return True\n            else:\n                print(\"â„¹ï¸  File already patched or pattern not found\")\n                return True\n    \n    print(\"âŒ Could not find sam2/utils/transforms.py in sys.path\")\n    return False\n\n# Run patch\npatch_success = patch_sam2_transforms()\n\nif patch_success:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"âœ… Patching completed! Safe to import SAM2 now.\")\n    print(\"=\" * 80)\nelse:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"âš ï¸  Patching failed. Consider using SAM1 instead.\")\n    print(\"=\" * 80)\n\n\n# =============================================================================\n# CELL 3: IMPORT LIBRARIES\n# =============================================================================\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport json\nfrom tqdm import tqdm\n\n# NOW it's safe to import SAM2 (after patching)\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\nfrom sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n\nprint(\"=\" * 80)\nprint(\"âœ… All libraries imported successfully!\")\nprint(f\"ðŸ“Š CUDA available: {torch.cuda.is_available()}\")\nprint(f\"ðŸ”§ TORCH_JIT: {os.environ.get('TORCH_JIT', 'not set')}\")\nprint(\"=\" * 80)\n\n\n# =============================================================================\n# CELL 4: STAGE 1 - EXTRACT LARGEST OBJECT WITH SAM2\n# =============================================================================\n\n# Configuration\nSAM2_CONFIG = \"sam2_hiera_l.yaml\"\nSAM2_CHECKPOINT = \"sam2_hiera_large.pt\"\n\ndef extract_largest_object_sam2_simple(\n    image_path,\n    model_cfg=SAM2_CONFIG,\n    checkpoint_path=SAM2_CHECKPOINT,\n    show=False\n):\n    \"\"\"\n    Extract largest object using SAM2 with point prompts.\n    More stable than automatic mask generation.\n    \n    Args:\n        image_path (str): Path to input image\n        model_cfg (str): SAM2 config file\n        checkpoint_path (str): SAM2 checkpoint path\n        show (bool): Display visualization\n    \n    Returns:\n        PIL.Image: Cropped object image\n    \"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Build SAM2 model\n    sam2_model = build_sam2(model_cfg, checkpoint_path, device=device)\n    predictor = SAM2ImagePredictor(sam2_model)\n    \n    # Read image\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Cannot read image: {image_path}\")\n    \n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    h, w = img.shape[:2]\n    \n    # Set image for predictor\n    predictor.set_image(img_rgb)\n    \n    # Create point prompts: center + 4 corners\n    input_points = np.array([\n        [w//2, h//2],      # center\n        [w//4, h//4],      # top-left region\n        [3*w//4, h//4],    # top-right region\n        [w//4, 3*h//4],    # bottom-left region\n        [3*w//4, 3*h//4],  # bottom-right region\n    ])\n    input_labels = np.ones(len(input_points), dtype=np.int32)  # all foreground\n    \n    # Predict masks\n    masks, scores, _ = predictor.predict(\n        point_coords=input_points,\n        point_labels=input_labels,\n        multimask_output=True\n    )\n    \n    # Handle no detection case\n    if len(masks) == 0:\n        print(\"âš ï¸ No object detected â€” returning original image\")\n        return Image.fromarray(img_rgb)\n    \n    # Select mask with highest confidence score\n    best_idx = np.argmax(scores)\n    best_mask = masks[best_idx]\n    \n    # Find bounding box of the mask\n    coords = np.argwhere(best_mask)\n    if len(coords) == 0:\n        print(\"âš ï¸ Empty mask â€” returning original image\")\n        return Image.fromarray(img_rgb)\n    \n    y1, x1 = coords.min(axis=0)\n    y2, x2 = coords.max(axis=0)\n    \n    # Crop image with mask applied\n    object_crop = img_rgb[y1:y2+1, x1:x2+1]\n    mask_crop = best_mask[y1:y2+1, x1:x2+1]\n    object_crop = object_crop * mask_crop[:, :, np.newaxis]\n    \n    # Visualization\n    if show:\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n        \n        axes[0].imshow(img_rgb)\n        axes[0].set_title(\"Original Image\")\n        axes[0].axis('off')\n        \n        axes[1].imshow(img_rgb)\n        axes[1].imshow(best_mask, alpha=0.5, cmap='jet')\n        axes[1].set_title(f\"Best Mask (score={scores[best_idx]:.3f})\")\n        axes[1].axis('off')\n        \n        axes[2].imshow(object_crop)\n        axes[2].set_title(\"Extracted Object\")\n        axes[2].axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    return Image.fromarray(object_crop.astype(np.uint8))\n\n\ndef extract_largest_object_sam2_auto(\n    image_path,\n    model_cfg=SAM2_CONFIG,\n    checkpoint_path=SAM2_CHECKPOINT,\n    show=False\n):\n    \"\"\"\n    Extract largest object using SAM2 automatic mask generation.\n    Slower but more accurate for complex scenes.\n    \n    Args:\n        image_path (str): Path to input image\n        model_cfg (str): SAM2 config file\n        checkpoint_path (str): SAM2 checkpoint path\n        show (bool): Display visualization\n    \n    Returns:\n        PIL.Image: Cropped object image\n    \"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Build SAM2 model\n    sam2_model = build_sam2(model_cfg, checkpoint_path, device=device)\n    mask_generator = SAM2AutomaticMaskGenerator(sam2_model)\n    \n    # Read image\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Cannot read image: {image_path}\")\n    \n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    # Generate all masks automatically\n    masks = mask_generator.generate(img_rgb)\n    \n    if len(masks) == 0:\n        print(\"âš ï¸ No object detected â€” returning original image\")\n        return Image.fromarray(img_rgb)\n    \n    # Find largest mask by area\n    largest_mask = max(masks, key=lambda x: x['area'])\n    segmentation = largest_mask['segmentation']\n    bbox = largest_mask['bbox']  # [x, y, w, h]\n    \n    x, y, w, h = map(int, bbox)\n    \n    # Crop with mask\n    object_crop = img_rgb[y:y+h, x:x+w]\n    mask_crop = segmentation[y:y+h, x:x+w]\n    object_crop = object_crop * mask_crop[:, :, np.newaxis]\n    \n    # Visualization\n    if show:\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n        \n        axes[0].imshow(img_rgb)\n        axes[0].set_title(\"Original Image\")\n        axes[0].axis('off')\n        \n        axes[1].imshow(img_rgb)\n        axes[1].imshow(segmentation, alpha=0.5, cmap='jet')\n        axes[1].set_title(f\"Largest Mask (area={largest_mask['area']})\")\n        axes[1].axis('off')\n        \n        axes[2].imshow(object_crop)\n        axes[2].set_title(\"Extracted Object\")\n        axes[2].axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n    \n    return Image.fromarray(object_crop.astype(np.uint8))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:17:23.526925Z","iopub.execute_input":"2025-11-18T11:17:23.527235Z","iopub.status.idle":"2025-11-18T11:17:23.717511Z","shell.execute_reply.started":"2025-11-18T11:17:23.527207Z","shell.execute_reply":"2025-11-18T11:17:23.716567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 2","metadata":{}},{"cell_type":"code","source":"## ----------------------STAGE 2-----------------------------------\ndef extract_main_object_caption(image_path, model_id=\"Salesforce/blip-image-captioning-base\", prompt=\"The single most important object in this image is a\"):\n    \"\"\"\n    XÃ¡c Ä‘á»‹nh váº­t thá»ƒ chÃ­nh trong áº£nh báº±ng mÃ´ hÃ¬nh BLIP image captioning.\n\n    Args:\n        image_path (str): ÄÆ°á»ng dáº«n áº£nh Ä‘áº§u vÃ o.\n        model_id (str): ID mÃ´ hÃ¬nh BLIP tá»« Hugging Face Hub.\n        prompt (str): Prompt mÃ´ táº£ váº­t thá»ƒ cáº§n tÃ¬m trong áº£nh.\n\n    Returns:\n        str: TÃªn váº­t thá»ƒ chÃ­nh Ä‘Æ°á»£c BLIP mÃ´ táº£.\n    \"\"\"\n\n    # Load model vÃ  processor\n    processor = BlipProcessor.from_pretrained(model_id)\n    blip_model = BlipForConditionalGeneration.from_pretrained(model_id)\n\n    # Má»Ÿ áº£nh\n    image = Image.open(image_path).convert(\"RGB\")\n\n    # Chuáº©n bá»‹ input\n    prompt = \"The single most important object in this image is a\" \n    inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n\n    # Sinh mÃ´ táº£ ngáº¯n\n    out = blip_model.generate(**inputs, max_new_tokens=5)\n\n    # Giáº£i mÃ£ output\n    generated_text = processor.decode(out[0], skip_special_tokens=True)\n\n    # TrÃ­ch xuáº¥t váº­t thá»ƒ\n    reference_object = (\n        generated_text.replace(prompt.lower(), \"\")\n        .replace(prompt, \"\")\n        .strip()\n        .split('.')[0]\n        .strip()\n    )\n    return reference_object\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:17:23.718762Z","iopub.execute_input":"2025-11-18T11:17:23.719084Z","iopub.status.idle":"2025-11-18T11:17:23.725805Z","shell.execute_reply.started":"2025-11-18T11:17:23.71905Z","shell.execute_reply":"2025-11-18T11:17:23.724823Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 3","metadata":{}},{"cell_type":"code","source":"def annotate_on_frame(image_path, text_prompt, box_threshold=0.35, text_threshold=0.25, max_crop_ratio=0.7):\n    \"\"\"\n    Annotates an input image with detected objects vÃ  cÃ¡c phrases tá»« GroundingDINO.\n    Giá»¯ nguyÃªn mÃ u áº£nh gá»‘c.\n\n    Parameters:\n    - max_crop_ratio: float, crop chiáº¿m tá»‰ lá»‡ > max_crop_ratio so vá»›i frame sáº½ bá»‹ loáº¡i bá»\n\n    Returns:\n    - annotated_frame (numpy.ndarray)\n    - abs_box (list of [x0,y0,x1,y1])\n    - logits\n    - phrases\n    - crops (list of numpy.ndarray)\n    \"\"\"\n    import torch\n    import numpy as np\n    import cv2\n\n    image_source, image = load_image(image_path)\n    ht, wd = image_source.shape[:2]\n\n    boxes, logits, phrases = predict(\n        model=dino_model,\n        image=image,\n        caption=text_prompt,\n        box_threshold=box_threshold,\n        text_threshold=text_threshold\n    )\n\n    abs_box = []\n    crops = []\n\n    for box, phrase in zip(boxes, phrases):\n        cx, cy, bw, bh = box.detach().cpu().numpy()\n        x0 = int(round((cx - bw/2) * wd))\n        y0 = int(round((cy - bh/2) * ht))\n        x1 = int(round((cx + bw/2) * wd))\n        y1 = int(round((cy + bh/2) * ht))\n\n        # Clip vÃ o áº£nh\n        x0 = max(0, min(x0, wd-1))\n        y0 = max(0, min(y0, ht-1))\n        x1 = max(0, min(x1, wd-1))\n        y1 = max(0, min(y1, ht-1))\n\n        # Loáº¡i bá» crop quÃ¡ lá»›n\n        crop_area = (x1 - x0) * (y1 - y0)\n        frame_area = ht * wd\n        if crop_area / frame_area > max_crop_ratio:\n            continue  # bá» crop nÃ y\n\n        abs_box.append([x0, y0, x1, y1])\n        crop = image_source[y0:y1, x0:x1].copy()\n        crops.append(crop)\n\n    # Váº½ bounding box lÃªn áº£nh gá»‘c\n    annotated_frame = image_source.copy()\n    for (x0, y0, x1, y1), phrase in zip(abs_box, phrases):\n        cv2.rectangle(annotated_frame, (x0, y0), (x1, y1), (255,0,0), 2)\n        cv2.putText(annotated_frame, phrase, (x0, max(y0-5,0)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n\n    return annotated_frame, abs_box, logits, phrases, crops\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:17:23.727068Z","iopub.execute_input":"2025-11-18T11:17:23.727323Z","iopub.status.idle":"2025-11-18T11:17:23.742717Z","shell.execute_reply.started":"2025-11-18T11:17:23.727279Z","shell.execute_reply":"2025-11-18T11:17:23.741852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stage 4","metadata":{}},{"cell_type":"code","source":"## ----------------------STAGE 4-----------------------------------\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport torch\nimport numpy as np\nimport cv2\n\n# ---------------- Load CLIP model once ----------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 0.508\n# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch16\").to(device)\n# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch16\")\n\n#0.3522\n# clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\").to(device)\n# processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n\n# clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\").to(device)\n# processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n\ndef compare_crops_with_reference(reference_img_array, crops):\n    \"\"\"\n    So sÃ¡nh nhiá»u crop vá»›i 1 reference image báº±ng CLIP (khÃ´ng hiá»ƒn thá»‹).\n\n    Parameters:\n    - reference_img_array: numpy array (H,W,3), BGR hoáº·c RGB\n    - crops: list of numpy arrays (H_crop,W_crop,3)\n\n    Returns:\n    - similarities: list cÃ¡c giÃ¡ trá»‹ similarity (rá»—ng náº¿u khÃ´ng cÃ³ crop)\n    - best_idx: index cá»§a crop match tá»‘t nháº¥t (None náº¿u khÃ´ng cÃ³ crop)\n    - best_similarity: similarity cao nháº¥t (None náº¿u khÃ´ng cÃ³ crop)\n    \"\"\"\n    if len(crops) == 0:\n        return [], None, None\n\n    # Chuyá»ƒn reference image sang RGB náº¿u cáº§n\n    if reference_img_array.shape[2] == 3:\n        reference_rgb = cv2.cvtColor(reference_img_array, cv2.COLOR_BGR2RGB) \\\n            if reference_img_array.dtype != np.float32 else reference_img_array\n    else:\n        reference_rgb = reference_img_array\n\n    reference_img = Image.fromarray(reference_rgb)\n    ref_inputs = processor(images=reference_img, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        ref_features = clip_model.get_image_features(**ref_inputs)\n        ref_features = ref_features / ref_features.norm(dim=-1, keepdim=True)\n\n    similarities = []\n\n    for crop in crops:\n        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB) if crop.shape[2] == 3 else crop\n        crop_pil = Image.fromarray(crop_rgb)\n        crop_inputs = processor(images=crop_pil, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():\n            crop_features = clip_model.get_image_features(**crop_inputs)\n            crop_features = crop_features / crop_features.norm(dim=-1, keepdim=True)\n\n        # Cosine similarity\n        sim = (ref_features @ crop_features.T).item()\n        similarities.append(sim)\n    if len(similarities) == 0:\n        return [], None, None\n\n    best_idx = int(np.argmax(similarities))\n    best_similarity = similarities[best_idx]\n\n    return similarities, best_idx, best_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:17:23.743799Z","iopub.execute_input":"2025-11-18T11:17:23.744028Z","iopub.status.idle":"2025-11-18T11:17:29.625262Z","shell.execute_reply.started":"2025-11-18T11:17:23.744006Z","shell.execute_reply":"2025-11-18T11:17:29.62419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_label_from_folder(folder_name):\n    # Loáº¡i bá» pháº§n sá»‘ vÃ  gáº¡ch dÆ°á»›i á»Ÿ cuá»‘i (vd: Backpack_0 -> Backpack)\n    label = re.sub(r'_\\d+$', '', folder_name)\n    # Thay tháº¿ gáº¡ch dÆ°á»›i giá»¯a cÃ¡c tá»« báº±ng khoáº£ng tráº¯ng (náº¿u cÃ³)\n    return label.replace('_', ' ')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom PIL import Image\nimport yaml\nimport re\nimport gc\n\n# --- Cáº¥u hÃ¬nh ---\nDATA_SOURCES = [\"/kaggle/input/train-zaic-dl\", \"/kaggle/input/zaic-test-frames\"]\nDISTILL_DIR = \"/kaggle/working/yolo_world_distill\"\nIMG_PATH = os.path.join(DISTILL_DIR, \"images\")\nLBL_PATH = os.path.join(DISTILL_DIR, \"labels\")\nos.makedirs(IMG_PATH, exist_ok=True)\nos.makedirs(LBL_PATH, exist_ok=True)\n\n# 1. Táº¡o bá»™ tá»« Ä‘iá»ƒn chuáº©n tá»« tÃªn cÃ¡c folder dá»¯ liá»‡u\nunique_labels = set()\nfor src in DATA_SOURCES:\n    if os.path.exists(src):\n        for folder in os.listdir(src):\n            if os.path.isdir(os.path.join(src, folder)):\n                unique_labels.add(get_label_from_folder(folder))\n\nclass_list = sorted(list(unique_labels)) \nlabel_map = {name: i for i, name in enumerate(class_list)}\nSIMILARITY_THRESHOLD = 0.65 \n\n# 2. VÃ²ng láº·p gÃ¡n nhÃ£n\nfor root_path in DATA_SOURCES:\n    if not os.path.exists(root_path): continue\n    \n    for folder_name in sorted(os.listdir(root_path)):\n        folder_path = os.path.join(root_path, folder_name)\n        if not os.path.isdir(folder_path): continue\n        \n        # NhÃ£n má»¥c tiÃªu cá»‘ Ä‘á»‹nh cho folder nÃ y (Äá»ƒ Student há»c Ä‘á»“ng nháº¥t)\n        target_label = get_label_from_folder(folder_name)\n        class_id = label_map[target_label]\n\n        # A. TEACHER PREPARATION (DÃ¹ng BLIP Ä‘á»ƒ láº¥y prompt mÃ´ táº£ cho DINO)\n        obj_images_dir = os.path.join(folder_path, \"object_images\")\n        ref_img_path = os.path.join(obj_images_dir, \"img_1.jpg\")\n        \n        ref_img_pil = extract_largest_object_sam2_simple(ref_img_path)\n        # Prompt nÃ y chá»‰ dÃ¹ng Ä‘á»ƒ DINO \"biáº¿t Ä‘Æ°á»ng mÃ  tÃ¬m\", khÃ´ng dÃ¹ng lÃ m nhÃ£n lÆ°u trá»¯\n        dino_prompt = extract_main_object_caption(ref_img_path)\n\n        gc.collect()\n        torch.cuda.empty_cache() #\n        \n        # B. AUTO-LABELING\n        frames_path = os.path.join(folder_path, \"object_frames\")\n        for frame_name in tqdm(os.listdir(frames_path), desc=f\"Distilling {folder_name}\"):\n            frame_p = os.path.join(frames_path, frame_name)\n            \n            # GroundingDINO tÃ¬m cÃ¡c vÃ¹ng kháº£ nghi theo mÃ´ táº£ cá»§a BLIP\n            _, abs_boxes, _, _, crops = annotate_on_frame(frame_p, dino_prompt)\n            \n            # CLIP lá»c láº¡i vÃ¹ng giá»‘ng nháº¥t vá»›i áº£nh máº«u (Few-shot logic)\n            sims, best_idx, best_sim = compare_crops_with_reference(np.array(ref_img_pil), crops)\n            \n            if best_idx is not None and best_sim >= SIMILARITY_THRESHOLD:\n                new_img_name = f\"{folder_name}_{frame_name}\"\n                frame_img = cv2.imread(frame_p)\n                \n                # LÆ°u nhÃ£n vá»›i class_id Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a tá»« folder\n                h, w = frame_img.shape[:2]\n                box = abs_boxes[best_idx]\n                \n                # Viáº¿t file nhÃ£n YOLO\n                with open(os.path.join(LBL_PATH, new_img_name.replace(\".jpg\", \".txt\")), \"w\") as f:\n                    cx, cy = (box[0] + box[2])/2/w, (box[1] + box[3])/2/h\n                    bw, bh = (box[2] - box[0])/w, (box[3] - box[1])/h\n                    f.write(f\"{class_id} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\\n\")\n                \n                cv2.imwrite(os.path.join(IMG_PATH, new_img_name), frame_img)\n\n# 3. Tá»± Ä‘á»™ng táº¡o file cáº¥u hÃ¬nh cho Student\nwith open('/kaggle/working/data.yaml', 'w') as f:\n    yaml.dump({'train': IMG_PATH, 'val': IMG_PATH, 'nc': len(class_list), 'names': class_list}, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}