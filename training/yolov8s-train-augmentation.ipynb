{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle URL: [Yolov8s Train Augmentation](https://www.kaggle.com/code/phatle1578/yolov8s-train-augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-26T14:35:10.929049Z",
     "iopub.status.busy": "2025-11-26T14:35:10.928775Z",
     "iopub.status.idle": "2025-11-26T14:38:21.710798Z",
     "shell.execute_reply": "2025-11-26T14:38:21.710011Z",
     "shell.execute_reply.started": "2025-11-26T14:35:10.929026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y ray ray[default] ray[tune] >/dev/null 2>&1 || true\n",
    "!pip -q install ultralytics==8.3.27 opencv-python==4.10.0.84 tqdm==4.67.1 torch==2.1.2 torchvision==0.16.2 open_clip_torch==2.24.0\n",
    "# üîß FIX l·ªói TensorBoard / protobuf conflict\n",
    "!pip install -q protobuf==3.20.3 tensorboard==2.14.0\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# ====== CONFIG ======\n",
    "TRAIN_ROOT = Path(\"/kaggle/input/dataset-dl-project/observing/train/samples\")\n",
    "ANN_PATH   = Path(\"/kaggle/input/dataset-dl-project/observing/train/annotations/annotations.json\")\n",
    "\n",
    "# Th∆∞ m·ª•c output\n",
    "WORK_ROOT      = Path(\"/kaggle/working\")\n",
    "YOLO_ROOT      = WORK_ROOT / \"yolo_data\"\n",
    "\n",
    "# Chia train/val theo video\n",
    "VAL_RATIO = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "ZOOM_IN_ENABLED = True\n",
    "ZOOM_SCALE = 1.2    # ph√≥ng to bbox l√™n ~1.8 l·∫ßn\n",
    "MAX_ZOOM_PER_FRAME = 3\n",
    "\n",
    "# ƒê·ªçc file annotations.json\n",
    "with open(ANN_PATH, \"r\") as f:\n",
    "    ann_data = json.load(f)   # list c√°c video\n",
    "\n",
    "# L·∫•y list video_id\n",
    "video_ids = [item[\"video_id\"] for item in ann_data]\n",
    "\n",
    "# Chia train/val theo video_id\n",
    "random.seed(RANDOM_SEED)\n",
    "video_ids_shuffled = video_ids.copy()\n",
    "random.shuffle(video_ids_shuffled)\n",
    "\n",
    "num_val = int(len(video_ids_shuffled) * VAL_RATIO)\n",
    "val_ids = set(video_ids_shuffled[:num_val])\n",
    "train_ids = set(video_ids_shuffled[num_val:])\n",
    "\n",
    "print(f\"S·ªë video train: {len(train_ids)}, val: {len(val_ids)}\")\n",
    "\n",
    "# Map video_id -> 'train' ho·∫∑c 'val'\n",
    "split_map = {}\n",
    "for vid in video_ids:\n",
    "    split_map[vid] = \"val\" if vid in val_ids else \"train\"\n",
    "\n",
    "def build_bboxes_per_frame(video_item):\n",
    "    \"\"\"\n",
    "    video_item: 1 dict trong ann_data\n",
    "    return: dict[frame_id] = list[bbox_dict]\n",
    "    \"\"\"\n",
    "    bboxes_per_frame = {}\n",
    "    for track in video_item[\"annotations\"]:   # m·ªói track l√† 1 'annotations'\n",
    "        for b in track[\"bboxes\"]:\n",
    "            frame = b[\"frame\"]\n",
    "            bboxes_per_frame.setdefault(frame, []).append(b)\n",
    "    return bboxes_per_frame\n",
    "\n",
    "# T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c YOLO\n",
    "for split in [\"train\", \"val\"]:\n",
    "    (YOLO_ROOT / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "    (YOLO_ROOT / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Th∆∞ m·ª•c Siamese (m·ªói object = 1 folder, ch·ª©a templates + patches)\n",
    "SIAMESE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def bbox_to_yolo_line(bbox, img_w, img_h, class_id=0):\n",
    "    x1, y1 = bbox[\"x1\"], bbox[\"y1\"]\n",
    "    x2, y2 = bbox[\"x2\"], bbox[\"y2\"]\n",
    "\n",
    "    xc = (x1 + x2) / 2.0\n",
    "    yc = (y1 + y2) / 2.0\n",
    "    bw = (x2 - x1)\n",
    "    bh = (y2 - y1)\n",
    "\n",
    "    # normalize\n",
    "    xc /= img_w\n",
    "    yc /= img_h\n",
    "    bw /= img_w\n",
    "    bh /= img_h\n",
    "\n",
    "    return f\"{class_id} {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\\n\"\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, img_w, img_h, scale=1.8):\n",
    "    \"\"\"\n",
    "    M·ªü r·ªông bbox quanh t√¢m theo factor scale.\n",
    "    Tr·∫£ v·ªÅ bbox m·ªõi (nx1, ny1, nx2, ny2) ƒë√£ clamp trong ·∫£nh.\n",
    "    \"\"\"\n",
    "    cx = (x1 + x2) / 2.0\n",
    "    cy = (y1 + y2) / 2.0\n",
    "    bw = (x2 - x1)\n",
    "    bh = (y2 - y1)\n",
    "\n",
    "    new_bw = bw * scale\n",
    "    new_bh = bh * scale\n",
    "\n",
    "    nx1 = cx - new_bw / 2.0\n",
    "    ny1 = cy - new_bh / 2.0\n",
    "    nx2 = cx + new_bw / 2.0\n",
    "    ny2 = cy + new_bh / 2.0\n",
    "\n",
    "    nx1 = max(0, int(nx1))\n",
    "    ny1 = max(0, int(ny1))\n",
    "    nx2 = min(img_w - 1, int(nx2))\n",
    "    ny2 = min(img_h - 1, int(ny2))\n",
    "\n",
    "    if nx2 <= nx1 or ny2 <= ny1:\n",
    "        return None  # bbox l·ªói\n",
    "\n",
    "    return nx1, ny1, nx2, ny2\n",
    "\n",
    "def process_one_video(video_item):\n",
    "    video_id = video_item[\"video_id\"]\n",
    "    split = split_map[video_id]   # 'train' ho·∫∑c 'val'\n",
    "\n",
    "    print(f\"Processing video {video_id} ({split})\")\n",
    "\n",
    "    # Build mapping frame -> list bboxes\n",
    "    bboxes_per_frame = build_bboxes_per_frame(video_item)\n",
    "    frames_to_keep = set(bboxes_per_frame.keys())\n",
    "\n",
    "    # ƒê∆∞·ªùng d·∫´n video & ·∫£nh template\n",
    "    obj_dir = TRAIN_ROOT / video_id\n",
    "    video_path = obj_dir / \"drone_video.mp4\"\n",
    "    template_dir = obj_dir / \"object_images\"   # 3 ·∫£nh object\n",
    "\n",
    "    # Th∆∞ m·ª•c output YOLO\n",
    "    img_out_dir   = YOLO_ROOT / \"images\" / split\n",
    "    label_out_dir = YOLO_ROOT / \"labels\" / split\n",
    "    \n",
    "    # M·ªü video v√† tr√≠ch frame\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"  >> WARNING: cannot open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_idx = 0\n",
    "    patch_count = 0\n",
    "\n",
    "    pbar = tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), desc=f\"{video_id}\", leave=False, dynamic_ncols=True)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx in frames_to_keep:\n",
    "            h, w = frame.shape[:2]\n",
    "\n",
    "            # T√™n file base (ƒë·∫£m b·∫£o l√† unique)\n",
    "            base_name = f\"{video_id}_{frame_idx:06d}\"\n",
    "\n",
    "            # 1) L∆∞u ·∫£nh cho YOLO\n",
    "            img_out_path = img_out_dir / f\"{base_name}.jpg\"\n",
    "            cv2.imwrite(str(img_out_path), frame)\n",
    "\n",
    "            # 2) T·∫°o label file YOLOs\n",
    "            label_lines = []\n",
    "            frame_bboxes = bboxes_per_frame[frame_idx]\n",
    "            for bbox in frame_bboxes:\n",
    "                line = bbox_to_yolo_line(bbox, w, h, class_id=0)\n",
    "                label_lines.append(line)\n",
    "\n",
    "            label_out_path = label_out_dir / f\"{base_name}.txt\"\n",
    "            with open(label_out_path, \"w\") as f:\n",
    "                f.writelines(label_lines)\n",
    "\n",
    "            if ZOOM_IN_ENABLED:\n",
    "                zoom_count = 0\n",
    "                for bbox_idx, bbox in enumerate(frame_bboxes):\n",
    "                    if zoom_count >= MAX_ZOOM_PER_FRAME:\n",
    "                        break\n",
    "                    x1, y1 = bbox[\"x1\"], bbox[\"y1\"]\n",
    "                    x2, y2 = bbox[\"x2\"], bbox[\"y2\"]\n",
    "\n",
    "                    expanded = expand_bbox(x1, y1, x2, y2, w, h, scale=ZOOM_SCALE)\n",
    "                    if expanded is None:\n",
    "                        continue\n",
    "                        \n",
    "                    ex1, ey1, ex2, ey2 = expanded\n",
    "                    crop = frame[ey1:ey2, ex1:ex2]\n",
    "                    ch, cw = crop.shape[:2]\n",
    "\n",
    "                    # bbox m·ªõi trong crop\n",
    "                    new_x1 = x1 - ex1\n",
    "                    new_y1 = y1 - ey1\n",
    "                    new_x2 = x2 - ex1\n",
    "                    new_y2 = y2 - ey1\n",
    "\n",
    "                    new_x1 = max(0, new_x1)\n",
    "                    new_y1 = max(0, new_y1)\n",
    "                    new_x2 = min(cw - 1, new_x2)\n",
    "                    new_y2 = min(ch - 1, new_y2)\n",
    "\n",
    "                    if new_x2 <= new_x1 or new_y2 <= new_y1:\n",
    "                        continue\n",
    "\n",
    "                    # l∆∞u ·∫£nh zoom-in\n",
    "                    zoom_name = f\"{video_id}_{frame_idx:06d}_z{bbox_idx}\"\n",
    "                    zoom_img_path = img_out_dir / f\"{zoom_name}.jpg\"\n",
    "                    cv2.imwrite(str(zoom_img_path), crop)\n",
    "\n",
    "                    # label YOLO cho ·∫£nh zoom-in (1 bbox)\n",
    "                    xc = (new_x1 + new_x2) / 2.0 / cw\n",
    "                    yc = (new_y1 + new_y2) / 2.0 / ch\n",
    "                    bw = (new_x2 - new_x1) / cw\n",
    "                    bh = (new_y2 - new_y1) / ch\n",
    "\n",
    "                    zoom_label_path = label_out_dir / f\"{zoom_name}.txt\"\n",
    "                    with open(zoom_label_path, \"w\") as fz:\n",
    "                        fz.write(f\"0 {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
    "\n",
    "                    zoom_count += 1\n",
    "\n",
    "        frame_idx += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T04:46:41.468151Z",
     "iopub.status.busy": "2025-11-25T04:46:41.467784Z",
     "iopub.status.idle": "2025-11-25T08:17:57.686987Z",
     "shell.execute_reply": "2025-11-25T08:17:57.686275Z",
     "shell.execute_reply.started": "2025-11-25T04:46:41.468131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "YOLO_ROOT = Path(\"/kaggle/working/yolo_data\")  # ch·ªânh n·∫øu kh√°c\n",
    "\n",
    "data_yaml = {\n",
    "    \"path\": str(YOLO_ROOT),   # th∆∞ m·ª•c g·ªëc ch·ª©a images/labels\n",
    "    \"train\": \"images/train\",  # relative path t·ª´ YOLO_ROOT\n",
    "    \"val\":   \"images/val\",\n",
    "    \"nc\": 1,\n",
    "    \"names\": [\"object\"]\n",
    "}\n",
    "\n",
    "with open(\"/kaggle/working/data.yaml\", \"w\") as f:\n",
    "    yaml.dump(data_yaml, f, sort_keys=False)\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")   # pretrain COCO\n",
    "\n",
    "model.train(\n",
    "    data=\"/kaggle/working/data.yaml\",\n",
    "    epochs=20,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    workers=4,\n",
    "    project=\"/kaggle/working/yolo_train\",\n",
    "    name=\"yolov8s_object\",\n",
    "    # augmentation (c√≥ th·ªÉ ch·ªânh)\n",
    "    augment=True,\n",
    "    fliplr=0.5,\n",
    "    scale=0.5,\n",
    "    degrees=10.0,\n",
    "    shear=2.0,\n",
    "    mosaic=0.5,\n",
    "    close_mosaic=5, \n",
    "    hsv_h=0.02,   # ƒë·ªïi hue nh·∫π\n",
    "    hsv_s=0.8,    # saturation m·∫°nh h∆°n (0.7‚Äì0.9)\n",
    "    hsv_v=0.5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8591239,
     "sourceId": 13533617,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
