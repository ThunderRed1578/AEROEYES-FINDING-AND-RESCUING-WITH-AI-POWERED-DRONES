{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13858516,"sourceType":"datasetVersion","datasetId":8828524},{"sourceId":648899,"sourceType":"modelInstanceVersion","modelInstanceId":489505,"modelId":504913},{"sourceId":706520,"sourceType":"modelInstanceVersion","modelInstanceId":536418,"modelId":549882},{"sourceId":706523,"sourceType":"modelInstanceVersion","modelInstanceId":536421,"modelId":549884},{"sourceId":707248,"sourceType":"modelInstanceVersion","modelInstanceId":537012,"modelId":550420}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Kaggle URL: [Pipeline Inference Drone](https://www.kaggle.com/code/phatle1578/pipeline-inference-drone)","metadata":{}},{"cell_type":"code","source":"!pip -q install ultralytics==8.3.27 open_clip_torch==2.24.0\n!pip uninstall -y ray ray[default] ray[tune] >/dev/null 2>&1 || true\n!pip install -q \"numpy<2.0\"\n!pip install -q protobuf==3.20.3 tensorboard==2.14.0\n!pip install -q filterpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:32:24.745875Z","iopub.execute_input":"2026-01-03T04:32:24.746100Z","execution_failed":"2026-01-03T04:32:54.581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os, json\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport requests\nimport math\nimport torch\nimport numpy as np\nimport open_clip\nfrom pathlib import Path\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-03T04:32:54.582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# PATH CONFIG\n# ============================================================\nSAMPLES_DIR = \"/kaggle/input/pt-zaic/private_test/samples\"\n\n# --- SỬA CHO ĐÚNG PATH MODEL CỦA BẠN ---\nYOLO_WORLD_WEIGHTS = \"/kaggle/input/yolo-world-distillation/transformers/default/1/best.pt\"\nYOLO11S_AUG_WEIGHTS = \"/kaggle/input/yolo11s-object/other/default/1/best.pt\"\nYOLOV8S_AUG_WEIGHTS = \"/kaggle/input/yolov8s-augmentation/transformers/default/1/best.pt\"\nSIAMESE_WEIGHTS    = \"/kaggle/input/clip-siamese-student-distillation/transformers/default/1/student_siamese_final.pt\"\n\nOUT_JSON = \"/kaggle/working/submission_private.json\"\nVIS_DIR  = \"/kaggle/working/vis_videos\"\nos.makedirs(VIS_DIR, exist_ok=True)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nFP16 = (DEVICE == \"cuda\")\n\n# ============================================================\n# KNOBS\n# ============================================================\nFRAME_STRIDE = 1\nIMG_SIZE = 640\nCONF_THRES = 0.2\nIOU_THRES = 0.5\nMAX_DETS = 50\nTOPK_PER_FRAME = 15\nCROP_PAD = 0.1\n\nSIM_THRES = 0.45\nFILL_SKIPPED_FRAMES = False\n\n# ============================================================\n# UTILS\n# ============================================================\ndef cv2_to_pil(img_bgr):\n    return Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n\ndef clamp_box(x1,y1,x2,y2,w,h):\n    x1 = max(0, min(int(x1), w-1))\n    y1 = max(0, min(int(y1), h-1))\n    x2 = max(0, min(int(x2), w-1))\n    y2 = max(0, min(int(y2), h-1))\n    if x2 <= x1: x2 = min(w-1, x1+1)\n    if y2 <= y1: y2 = min(h-1, y1+1)\n    return x1,y1,x2,y2\n\ndef pad_box(x1,y1,x2,y2,pad_frac,w,h):\n    bw = x2-x1\n    bh = y2-y1\n    padw = bw * pad_frac\n    padh = bh * pad_frac\n    return clamp_box(x1-padw, y1-padh, x2+padw, y2+padh, w, h)\n\ndef cosine_sim(a, b):\n    return (a @ b.transpose(0,1)).squeeze(0)\n\n# ============================================================\n# BLIP\n# ============================================================\nclass BLIPPrompter:\n    def __init__(self, model_id=\"Salesforce/blip-image-captioning-base\", device=DEVICE):\n        self.processor = BlipProcessor.from_pretrained(model_id)\n        self.model = BlipForConditionalGeneration.from_pretrained(model_id).to(device).eval()\n        self.device = device\n\n    @torch.inference_mode()\n    def caption(self, pil_img, prompt=\"The single most important object in this image is a\", max_new_tokens=6):\n        inputs = self.processor(images=pil_img, text=prompt, return_tensors=\"pt\").to(self.device)\n        out = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n        txt = self.processor.decode(out[0], skip_special_tokens=True)\n        ref = txt.lower().replace(prompt.lower(), \"\").strip()\n        ref = ref.split(\".\")[0].strip()\n        ref = ref.replace(\"a \", \"\").replace(\"an \", \"\").strip()\n        return ref\n\ndef build_prompts_from_ref_images(ref_paths, blip: BLIPPrompter, debug=False, case_id=\"\"):\n    base_prompt = \"The single most important object in this image is a\"\n    caps = []\n    for p in ref_paths:\n        img = Image.open(p).convert(\"RGB\")\n        c = blip.caption(img, prompt=base_prompt, max_new_tokens=6)\n        if c:\n            caps.append(c)\n        if debug:\n            print(f\"[BLIP][{case_id}] {Path(p).name} -> '{c}'\")\n\n    def clean(s):\n        s = s.strip().lower()\n        s = s.replace(\",\", \" \").replace(\"  \", \" \")\n        return s\n\n    caps = [clean(c) for c in caps if len(c.strip()) >= 2]\n    caps = list(dict.fromkeys(caps))\n\n    prompts = []\n    for c in caps:\n        prompts.append(c)\n        toks = c.split()\n        if len(toks) >= 2:\n            prompts.append(toks[-1])\n    prompts = [p for p in prompts if p]\n    prompts = list(dict.fromkeys(prompts))\n\n    if debug:\n        print(f\"[PROMPTS][{case_id}] {prompts}\")\n\n    return prompts if prompts else [\"object\"]\n\n# ============================================================\n# SIAMESE/CLIP embedder\n# ============================================================\nclass ClipEmbedder:\n    def __init__(self, siamese_path=SIAMESE_WEIGHTS, device=DEVICE):\n        self.device = device\n        self.model = None\n        self.preprocess = None\n        self.using_fallback = False\n\n        # 1) TorchScript\n        try:\n            m = torch.jit.load(siamese_path, map_location=device)\n            m.eval()\n            self.model = m\n            self.preprocess = None\n            print(\"[Siamese] Loaded as TorchScript:\", siamese_path)\n            return\n        except Exception as e:\n            print(\"[Siamese] TorchScript load failed:\", type(e).__name__)\n\n        # 2) state_dict -> open_clip (có thể cần đổi arch)\n        try:\n            arch = \"ViT-B-32\"\n            pretrained = \"laion2b_s34b_b79k\"\n            model, _, preprocess = open_clip.create_model_and_transforms(arch, pretrained=pretrained)\n            ckpt = torch.load(siamese_path, map_location=\"cpu\")\n            sd = ckpt.get(\"state_dict\", ckpt)\n            new_sd = {k.replace(\"module.\", \"\"): v for k, v in sd.items()}\n            model.load_state_dict(new_sd, strict=False)\n            self.model = model.to(device).eval()\n            self.preprocess = preprocess\n            print(\"[Siamese] Loaded as open_clip state_dict:\", siamese_path)\n            return\n        except Exception as e:\n            print(\"[Siamese] state_dict load failed:\", type(e).__name__)\n\n        # 3) fallback\n        arch = \"ViT-B-32\"\n        pretrained = \"laion2b_s34b_b79k\"\n        model, _, preprocess = open_clip.create_model_and_transforms(arch, pretrained=pretrained)\n        self.model = model.to(device).eval()\n        self.preprocess = preprocess\n        self.using_fallback = True\n        print(\"[Siamese] Fallback to open_clip pretrained:\", arch, pretrained)\n\n    @torch.inference_mode()\n    def encode_image(self, pil_img: Image.Image):\n        if self.preprocess is not None:\n            x = self.preprocess(pil_img).unsqueeze(0).to(self.device)\n        else:\n            img = pil_img.resize((224,224))\n            arr = np.array(img).astype(np.float32) / 255.0\n            mean = np.array([0.48145466, 0.4578275, 0.40821073], dtype=np.float32)\n            std  = np.array([0.26862954, 0.26130258, 0.27577711], dtype=np.float32)\n            arr = (arr - mean) / std\n            x = torch.from_numpy(arr).permute(2,0,1).unsqueeze(0).to(self.device)\n    \n        # ---- FIX: dùng autocast thay vì x.half() ----\n        if self.device.startswith(\"cuda\"):\n            with torch.cuda.amp.autocast(dtype=torch.float16):\n                if hasattr(self.model, \"encode_image\"):\n                    feat = self.model.encode_image(x)\n                else:\n                    feat = self.model(x)\n        else:\n            if hasattr(self.model, \"encode_image\"):\n                feat = self.model.encode_image(x)\n            else:\n                feat = self.model(x)\n    \n        feat = feat.float()\n        feat = feat / (feat.norm(dim=-1, keepdim=True) + 1e-8)\n        return feat\n\n# ============================================================\n# LIST CASES (names are duplicated across folders -> use folder name as video_id)\n# ============================================================\ndef list_cases(samples_dir):\n    samples_dir = Path(samples_dir)\n    cases = []\n    for d in sorted(samples_dir.iterdir()):\n        if not d.is_dir():\n            continue\n\n        video = d / \"drone_video.mp4\"\n        if not video.exists():\n            vids = list(d.glob(\"*.mp4\"))\n            if len(vids) == 0:\n                continue\n            video = vids[0]\n\n        obj_dir = d / \"object_images\"   # <-- đúng như bạn chụp\n        refs = []\n        if obj_dir.exists():\n            for ext in [\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.webp\"]:\n                refs += sorted(obj_dir.glob(ext))\n        refs = [str(x) for x in refs][:3]\n\n        cases.append({\"case_id\": d.name, \"video\": str(video), \"refs\": refs})\n    return cases\n\n# ============================================================\n# INFER ONE CASE -> submission entry + frame_to_box for visualization\n# ============================================================\ndef split_into_intervals(frame_to_box, gap_tolerance=1):\n    \"\"\"\n    gap_tolerance=1: chỉ coi là liên tiếp nếu frame sau = frame trước + 1.\n    \"\"\"\n    if not frame_to_box:\n        return []\n\n    frames = sorted(frame_to_box.keys())\n    detections = []\n\n    cur = []\n    prev = None\n\n    for f in frames:\n        if prev is None or (f - prev) <= gap_tolerance:\n            cur.append(f)\n        else:\n            # đóng interval cũ\n            bboxes = []\n            for ff in cur:\n                x1,y1,x2,y2 = frame_to_box[ff]\n                bboxes.append({\"frame\": int(ff), \"x1\": int(x1), \"y1\": int(y1), \"x2\": int(x2), \"y2\": int(y2)})\n            detections.append({\"bboxes\": bboxes})\n            # mở interval mới\n            cur = [f]\n        prev = f\n\n    # đóng interval cuối\n    if cur:\n        bboxes = []\n        for ff in cur:\n            x1,y1,x2,y2 = frame_to_box[ff]\n            bboxes.append({\"frame\": int(ff), \"x1\": int(x1), \"y1\": int(y1), \"x2\": int(x2), \"y2\": int(y2)})\n        detections.append({\"bboxes\": bboxes})\n\n    return detections\n    \ndef infer_one_case(case, yolo_world_model, yolo11s_aug, yolov8s_aug, blip, embedder: ClipEmbedder):\n    case_id = case[\"case_id\"]\n    video_path = case[\"video\"]\n    ref_paths = case[\"refs\"]\n\n    video_id = case_id  # unique id theo folder name\n\n    # ---- BLIP prompts (log console) ----\n    DEBUG_BLIP = True  # đổi True nếu muốn in hết; khuyên debug 1-2 case thôi\n    # DEBUG_BLIP = (video_id == \"Helmet_0\")\n    prompts = build_prompts_from_ref_images(ref_paths, blip, debug=DEBUG_BLIP, case_id=video_id)\n\n    # ---- reference embedding (mean of 3 refs) ----\n    ref_feats = []\n    for p in ref_paths:\n        if os.path.exists(p):\n            ref_img = Image.open(p).convert(\"RGB\")\n            ref_feats.append(embedder.encode_image(ref_img))\n\n    if len(ref_feats) == 0:\n        return {\"video_id\": video_id, \"detections\": []}, {}\n\n    ref_feat = torch.cat(ref_feats, dim=0).mean(dim=0, keepdim=True)\n    ref_feat = ref_feat / (ref_feat.norm(dim=-1, keepdim=True) + 1e-8)\n\n    # ---- video loop ----\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        return {\"video_id\": video_id, \"detections\": []}, {}\n\n    nframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_to_box = {}\n    last_good = None\n\n    frame_idx = 0\n    pbar = tqdm(total=nframes, desc=f\"[{video_id}]\")\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        h, w = frame.shape[:2]\n\n        # 1. Logic Stride: Bỏ qua frame theo bước nhảy để tăng tốc độ\n        if frame_idx % FRAME_STRIDE != 0:\n            if FILL_SKIPPED_FRAMES and (last_good is not None):\n                frame_to_box[frame_idx] = last_good[1]\n            frame_idx += 1\n            pbar.update(1)\n            continue\n\n        # 2. CHẠY SONG SONG 2 MÔ HÌNH STUDENT\n        # Model 1: YOLO-World (Open-Vocabulary - Nhận diện theo Prompt văn bản)\n        res_world = yolo_world.predict(frame, prompts=prompts, imgsz=IMG_SIZE, conf=CONF_THRES, iou=IOU_THRES, verbose=False)[0]\n        \n        # Model 2: YOLO11s (Fine-tuned - Tinh tường bối cảnh drone thực tế)\n        res_yolo11 = yolo11s_aug.predict(frame, imgsz=IMG_SIZE, conf=CONF_THRES, iou=IOU_THRES, verbose=False)[0]\n\n        # Model 3: YOLOv8s (Fine-tuned - Tinh tường bối cảnh drone thực tế)\n        res_yolov8 = yolov8s_aug.predict(frame, imgsz=IMG_SIZE, conf=CONF_THRES, iou=IOU_THRES, verbose=False)[0]\n\n        # 3. GOM KẾT QUẢ TỪ CẢ 2 NGUỒN (Ensemble Proposals)\n        all_boxes = []\n        all_confs = []\n        \n        if res_world.boxes is not None and len(res_world.boxes) > 0:\n            all_boxes.append(res_world.boxes.xyxy.cpu().numpy())\n            all_confs.append(res_world.boxes.conf.cpu().numpy())\n            \n        if res_yolo11.boxes is not None and len(res_yolo11.boxes) > 0:\n            all_boxes.append(res_yolo11.boxes.xyxy.cpu().numpy())\n            all_confs.append(res_yolo11.boxes.conf.cpu().numpy())\n\n        if res_yolov8.boxes is not None and len(res_yolov8.boxes) > 0:\n            all_boxes.append(res_yolov8.boxes.xyxy.cpu().numpy())\n            all_confs.append(res_yolov8.boxes.conf.cpu().numpy())\n\n        best_box = None\n        best_sim = -1.0\n\n        if len(all_boxes) > 0:\n            # Hợp nhất các mảng box và confidence\n            combined_boxes = np.concatenate(all_boxes)\n            combined_confs = np.concatenate(all_confs)\n            \n            # Sắp xếp và chỉ giữ lại TOP-K vùng khả nghi nhất để Siamese xử lý\n            order = np.argsort(-combined_confs)[:TOPK_PER_FRAME]\n            final_proposals = combined_boxes[order]\n\n            crop_feats = []\n            crop_boxes = []\n\n            # 4. XÁC MINH CHI TIẾT BẰNG SIAMESE STUDENT\n            for (x1, y1, x2, y2) in final_proposals:\n                # Mở rộng vùng cắt để lấy thêm bối cảnh\n                x1, y1, x2, y2 = pad_box(x1, y1, x2, y2, CROP_PAD, w, h)\n                crop = frame[y1:y2, x1:x2]\n                if crop.size == 0:\n                    continue\n\n                # Trích xuất đặc trưng hình ảnh của vùng cắt\n                feat = embedder.encode_image(cv2_to_pil(crop))\n                crop_feats.append(feat)\n                crop_boxes.append([int(x1), int(y1), int(x2), int(y2)])\n\n            if len(crop_feats) > 0:\n                # Tính độ tương đồng Cosine với ảnh tham chiếu\n                crop_feats = torch.cat(crop_feats, dim=0)\n                sims = cosine_sim(ref_feat, crop_feats).detach().cpu().numpy()\n\n                # Chọn box có độ tương đồng cao nhất với vật thể mẫu\n                bi = int(np.argmax(sims))\n                best_sim = float(sims[bi])\n                best_box = crop_boxes[bi]\n\n        # 5. CHẤP NHẬN HOẶC TỪ CHỐI DỰA TRÊN NGƯỠNG SIMILARITY\n        if (best_box is not None) and (best_sim >= SIM_THRES):\n            frame_to_box[frame_idx] = best_box\n            last_good = (frame_idx, best_box)\n        else:\n            # Nếu không tìm thấy, có thể điền bằng kết quả của frame trước đó\n            if FILL_SKIPPED_FRAMES and (last_good is not None):\n                frame_to_box[frame_idx] = last_good[1]\n\n        frame_idx += 1\n        pbar.update(1)\n\n    pbar.close()\n    cap.release()\n\n    # ---- IMPORTANT: build output by intervals (đúng format BTC) ----\n    detections = split_into_intervals(frame_to_box, gap_tolerance=1)\n    entry = {\"video_id\": video_id, \"detections\": detections}\n\n    return entry, frame_to_box\n\n# ============================================================\n# VISUALIZE VIDEO\n# ============================================================\ndef write_visualized_video(video_path, frame_to_box, out_path):\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(\"Cannot open for vis:\", video_path)\n        return\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(out_path, fourcc, fps, (w, h))\n\n    idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if idx in frame_to_box:\n            x1,y1,x2,y2 = frame_to_box[idx]\n            cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n            cv2.putText(frame, f\"{Path(out_path).stem}  f={idx}\", (10,30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2)\n\n        writer.write(frame)\n        idx += 1\n\n    cap.release()\n    writer.release()\n\n# ============================================================\n# RUN ALL\n# ============================================================\nyolo_world = YOLO(YOLO_WORLD_WEIGHTS)\nyolo11s_aug = YOLO(YOLO11S_AUG_WEIGHTS)\nyolov8s_aug = YOLO(YOLOV8S_AUG_WEIGHTS)\nblip = BLIPPrompter(device=DEVICE)\nembedder = ClipEmbedder(siamese_path=SIAMESE_WEIGHTS, device=DEVICE)\n\ncases = list_cases(SAMPLES_DIR)\nprint(\"Found cases:\", len(cases))\nprint(\"First case:\", cases[0] if cases else None)\n\nsubmission = []\nfor case in cases:\n    entry, frame_to_box = infer_one_case(case, yolo_world, yolo11s_aug, yolov8s_aug, blip, embedder)\n    submission.append(entry)\n\n    # save vis video (same folder name -> unique)\n    out_video = os.path.join(VIS_DIR, f\"{entry['video_id']}_vis.mp4\")\n    write_visualized_video(case[\"video\"], frame_to_box, out_video)\n\nwith open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n    json.dump(submission, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved JSON:\", OUT_JSON)\nprint(\"Saved VIS videos folder:\", VIS_DIR)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-03T04:32:54.582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r videos.zip $VIS_DIR","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-03T04:32:54.582Z"}},"outputs":[],"execution_count":null}]}